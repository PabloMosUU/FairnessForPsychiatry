{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "#matplot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#data frame to image\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# LIME\n",
    "from aif360.datasets.lime_encoder import LimeEncoder\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "DATA_DIR = '/media/bigdata/10. Stages/3. Afgerond/2020-08 Jesse Kuiper/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "#load data\n",
    "Dataset14Days = pd.read_csv(DATA_DIR + \"Dataset14Days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetwhole\n",
    "Dataset1 = pd.DataFrame(data=Dataset14Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corr plot of all variables\n",
    "corr_mat = Dataset1.corr()\n",
    "\n",
    "plt.subplots(figsize=(50,50))\n",
    "sns.heatmap(corr_mat, annot = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the dataset into an AIF360 dataset\n",
    "def get_favourable1(DoseDiazepamPost):\n",
    "    return DoseDiazepamPost==0\n",
    "\n",
    "#dataset = aif360.datasets.StandardDataset(\n",
    "#    your_pandas_df,\n",
    "#    label_name,\n",
    "#    favorable_classes,\n",
    "#    protected_attribute_names,\n",
    "#    privileged_classes)\n",
    "\n",
    "DW = StandardDataset(\n",
    "    Dataset1,\n",
    "    \"DoseDiazepamPost\",\n",
    "    get_favourable1,\n",
    "    [\"Geslacht\"],\n",
    "    [[\"Man\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LR and RF with unchanged data\n",
    "\n",
    "# split the date into 3 datasets\n",
    "(dataset_train,\n",
    " dataset_val,\n",
    " dataset_test) = DW.split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe function\n",
    "\n",
    "def describe(train=None, val=None, test=None):\n",
    "    if train is not None:\n",
    "        display(Markdown(\"#### Training Dataset shape\"))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown(\"#### Validation Dataset shape\"))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown(\"#### Test Dataset shape\"))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown(\"#### Protected attribute names\"))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "    print(test.privileged_protected_attributes, \n",
    "          test.unprivileged_protected_attributes)\n",
    "    display(Markdown(\"#### Dataset feature names\"))\n",
    "    print(test.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(dataset_train, dataset_val, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the disparate impact\n",
    "metric_train = BinaryLabelDatasetMetric(\n",
    "        dataset_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_train = MetricTextExplainer(metric_train)\n",
    "\n",
    "print(explainer_train.disparate_impact())\n",
    "\n",
    "#error is probably due to the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "\n",
    "lr_dataset = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr):\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        print('aif360 inprocessing algorithm')\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "    \n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "        metric = ClassificationMetric(\n",
    "                dataset, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "        \n",
    "        # calculate the F1 score\n",
    "        \n",
    "        metric_arrs['bal_acc'].append((metric.true_positive_rate()\n",
    "                                     + metric.true_negative_rate()) / 2)\n",
    "        metric_arrs['F1_score'].append(metric.true_positive_rate() /\n",
    "                                      (metric.true_positive_rate() + (0.5 *(metric.false_positive_rate() + metric.false_negative_rate())))) \n",
    "        \n",
    "        metric_arrs['FP Diff'].append(metric.false_positive_rate_difference())\n",
    "                                        \n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "        #metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.1, 0.9, 50)\n",
    "val_metrics = test(dataset=dataset_val,\n",
    "                   model=lr_dataset,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, x_name, y_left, y_left_name, y_right, y_right_name, filename=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(10,7))\n",
    "    ax1.plot(x, y_left)\n",
    "    ax1.set_xlabel(x_name, fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel(y_left_name, color='b', fontsize=16, fontweight='bold')\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    ax1.set_ylim(0.5, 1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, y_right, color='r')\n",
    "    ax2.set_ylabel(y_right_name, color='r', fontsize=16, fontweight='bold')\n",
    "    if 'DI' in y_right_name:\n",
    "        ax2.set_ylim(0., 1)\n",
    "    else:\n",
    "        ax2.set_ylim(-0.25, 1)\n",
    "\n",
    "    best_ind = np.argmax(y_left)\n",
    "    ax2.axvline(np.array(x)[best_ind], color='k', linestyle=':')\n",
    "    ax2.yaxis.set_tick_params(labelsize=14)\n",
    "    ax2.grid(True)\n",
    "    if filename:\n",
    "        fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imp = np.array(val_metrics['disp_imp'])\n",
    "disp_imp_err = 1 - np.minimum(disp_imp, 1/disp_imp)\n",
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     disp_imp_err, '1 - min(DI, 1/DI)',\n",
    "    filename='LR_DI_none.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     val_metrics['avg_odds_diff'], 'avg. odds diff.',\n",
    "    filename='LR_AOD_none.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_metrics(metrics, thresh_arr):\n",
    "    best_ind = np.argmax(metrics['bal_acc'])\n",
    "    print(\"Threshold corresponding to Best balanced accuracy: {:6.4f}\".format(thresh_arr[best_ind]))\n",
    "    print(\"Best balanced accuracy: {:6.4f}\".format(metrics['bal_acc'][best_ind]))\n",
    "#     disp_imp_at_best_ind = np.abs(1 - np.array(metrics['disp_imp']))[best_ind]\n",
    "    disp_imp_at_best_ind = 1 - min(metrics['disp_imp'][best_ind], 1/metrics['disp_imp'][best_ind])\n",
    "    print(\"F1 score: {:6.4f}\".format(metrics['F1_score'][best_ind]))\n",
    "    print(\"FP Diff: {:6.4f}\".format(metrics['FP Diff'][best_ind]))\n",
    "    print(\"Corresponding 1-min(DI, 1/DI) value: {:6.4f}\".format(disp_imp_at_best_ind))\n",
    "    print(\"Corresponding average odds difference value: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind]))\n",
    "    print(\"Corresponding statistical parity difference value: {:6.4f}\".format(metrics['stat_par_diff'][best_ind]))\n",
    "    print(\"Corresponding equal opportunity difference value: {:6.4f}\".format(metrics['eq_opp_diff'][best_ind]))\n",
    "    print('Corresponding disparate impact value: {:6.4f}'.format(metrics['disp_imp'][best_ind]))\n",
    "    #print(\"Corresponding Theil index value: {:6.4f}\".format(metrics['theil_ind'][best_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with normal data\n",
    "lr_orig_metrics = test(dataset=dataset_test,\n",
    "                       model=lr_dataset,\n",
    "                       thresh_arr=[thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(lr_orig_metrics, [thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forrest on original data\n",
    "\n",
    "dataset = dataset_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_orig_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF validating\n",
    "thresh_arr = np.linspace(0.1, 0.8, 50)\n",
    "val_metrics = test(dataset=dataset_val,\n",
    "                   model=rf_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "rf_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imp = np.array(val_metrics['disp_imp'])\n",
    "disp_imp_err = 1 - np.minimum(disp_imp, 1/disp_imp)\n",
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     disp_imp_err, '1 - min(DI, 1/DI)',\n",
    "    filename='RF_DI_none.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     val_metrics['avg_odds_diff'], 'avg. odds diff.',\n",
    "    filename='RF_AOD_none.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rf model\n",
    "rf_orig_metrics = test(dataset=dataset_test,\n",
    "                       model=rf_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(rf_orig_metrics, [thresh_arr[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data by reweighing\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(\n",
    "        dataset_transf_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_transf_train = MetricTextExplainer(metric_transf_train)\n",
    "\n",
    "print(explainer_transf_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training LR on reweight data\n",
    "dataset = dataset_transf_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "lr_transf = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "thresh_arr = np.linspace(0.01, 0.8, 50)\n",
    "val_metrics = test(dataset=dataset_val,\n",
    "                   model=lr_transf,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imp = np.array(val_metrics['disp_imp'])\n",
    "disp_imp_err = 1 - np.minimum(disp_imp, 1/disp_imp)\n",
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     disp_imp_err, '1 - min(DI, 1/DI)',\n",
    "    filename='LR_DI_reweigh.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     val_metrics['avg_odds_diff'], 'avg. odds diff.',\n",
    "    filename='LR_AOD_reweigh.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_transf_metrics = test(dataset=dataset_test,\n",
    "                         model=lr_transf,\n",
    "                         thresh_arr=[thresh_arr[lr_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(lr_transf_metrics, [thresh_arr[lr_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training RF on reweight data\n",
    "\n",
    "dataset = dataset_transf_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_transf = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.8, 50)\n",
    "val_metrics = test(dataset=dataset_val,\n",
    "                   model=rf_transf,\n",
    "                   thresh_arr=thresh_arr)\n",
    "rf_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imp = np.array(val_metrics['disp_imp'])\n",
    "disp_imp_err = 1 - np.minimum(disp_imp, 1/disp_imp)\n",
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     disp_imp_err, '1 - min(DI, 1/DI)',\n",
    "    filename='RF_DI_reweigh.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     val_metrics['avg_odds_diff'], 'avg. odds diff.',\n",
    "    filename='RF_AOD_reweigh.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_transf_metrics = test(dataset=dataset_test,\n",
    "                         model=rf_transf,\n",
    "                         thresh_arr=[thresh_arr[rf_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(rf_transf_metrics, [thresh_arr[rf_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inprocess mitigation - prejudice remover\n",
    "model = PrejudiceRemover(sensitive_attr=sens_attr, eta=25.0)\n",
    "pr_orig_scaler = StandardScaler()\n",
    "\n",
    "dataset = dataset_train.copy()\n",
    "dataset.features = pr_orig_scaler.fit_transform(dataset.features)\n",
    "\n",
    "pr_orig_DW = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.8, 50)\n",
    "\n",
    "dataset = dataset_val.copy()\n",
    "dataset.features = pr_orig_scaler.transform(dataset.features)\n",
    "\n",
    "val_metrics = test(dataset=dataset,\n",
    "                   model=pr_orig_DW,\n",
    "                   thresh_arr=thresh_arr)\n",
    "pr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imp = np.array(val_metrics['disp_imp'])\n",
    "disp_imp_err = 1 - np.minimum(disp_imp, 1/disp_imp)\n",
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     disp_imp_err, '1 - min(DI, 1/DI)',\n",
    "    filename='LR_DI_PR.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(thresh_arr, 'Classification Thresholds',\n",
    "     val_metrics['bal_acc'], 'Balanced Accuracy',\n",
    "     val_metrics['avg_odds_diff'], 'avg. odds diff.',\n",
    "    filename='LR_AOD_PR.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_test.copy()\n",
    "dataset.features = pr_orig_scaler.transform(dataset.features)\n",
    "\n",
    "pr_orig_metrics = test(dataset=dataset,\n",
    "                       model=pr_orig_DW,\n",
    "                       thresh_arr=[thresh_arr[pr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(pr_orig_metrics, [thresh_arr[pr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#table summary of the results\n",
    "pd.set_option('display.multi_sparse', False)\n",
    "results = [lr_orig_metrics, rf_orig_metrics, lr_transf_metrics,\n",
    "           rf_transf_metrics, pr_orig_metrics]\n",
    "debias = pd.Series(['']*2 + ['Reweighing']*2\n",
    "                 + ['Prejudice Remover'],\n",
    "                   name='Bias Mitigator')\n",
    "clf = pd.Series(['Logistic Regression', 'Random Forest']*2 + [''],\n",
    "                name='Classifier')\n",
    "results_df = pd.concat([pd.DataFrame(metrics) for metrics in results], axis=0).set_index([debias, clf])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at this by eye, and it's the same as was published in Jesse's thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.rename(columns={'bal_acc': 'Bal acc', 'F1_score': 'F1 score', 'disp_imp': 'Disp imp', \n",
    "                           'avg_odds_diff': 'Avg odds diff', 'stat_par_diff': 'Stat par diff', \n",
    "                           'eq_opp_diff': 'Eq opp diff'}, inplace=True)\n",
    "results_df.drop(columns=['FP Diff'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index(inplace=True)\n",
    "results_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = results_df.columns\n",
    "print(' & '.join(cols) + ' \\\\\\\\')\n",
    "print('\\\\hline')\n",
    "print('\\\\hline')\n",
    "for elll in [' & '.join(['{:.3f}'.format(ell) if type(ell) != str else ell for ell in row]) + ' \\\\\\\\' for row in results_df[[el for el in cols]].values]:\n",
    "    print(elll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
