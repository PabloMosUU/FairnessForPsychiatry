{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "DATA_DIR = '/media/bigdata/10. Stages/3. Afgerond/2020-08 Jesse Kuiper/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load opnamens\n",
    "opnamen = pd.read_csv(DATA_DIR + \"werkbestanden-opnames/latest/werkbestand_opnames.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load administered\n",
    "Toedienen = pd.read_csv(DATA_DIR + \"werkbestanden-medicatie/latest/werkbestand_medicatie_toediening.csv\", sep=';',decimal=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dbc\n",
    "dbcgeheel = pd.read_csv(DATA_DIR + \"werkbestanden-dbc/latest/werkbestand_dbc.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load map\n",
    "boos = pd.read_csv(DATA_DIR + \"werkbestanden-map/latest/werkbestand_map.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load patient or patient uniek\n",
    "pat = pd.read_csv(DATA_DIR + \"werkbestanden-patient/latest/werkbestand_patient_uniek.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting usefull var \n",
    "# what to do with \"AfdelingOmschrijving\" - just the most occuring ones\n",
    "admission = opnamen[[\"OpnameID\", \"PseudoID\",\"AfdelingOmschrijving\", \"Opnamedatum\",  \"Ontslagdatum\",  \"Opnametijd\", \n",
    "\"Ontslagtijd\",    \"Spoed\",  \"EersteOpname\",  \"Geslacht\",\n",
    "\"Leeftijd_opname\", \"OpnamestatusOmschrijving\", \"Duur\",]]\n",
    "\n",
    "administering = Toedienen[[\"PseudoID\", \"VoorschriftID\", \"ATC_code_omschr\", \"Medicijnnaam_ingevuld\",\"Dosis\", \"Eenheid\", \"ToedienDatum\", \"ToedienTijd\", \"Toegediend\", \"DosisVerbruikt\", \"DosisOrigineel\", \"ToedieningIsOpgeschort\", \"NietToegediend\"  ]]\n",
    "\n",
    "dbc = dbcgeheel[[ \"PseudoID\", \"dbcnummer\",\"Startdatum\", \"Einddatum\" ,\"hoofddiagnose_groep\", \"zorgvraagzwaarte\", \"MeervoudigeProblematiekInd\",\n",
    "                            \"persoonlijkheidsstoornis\", \"Opname\", \"DiagnoseDatum\" ]]\n",
    "\n",
    "violent = boos[[\"PseudoID\", \"hantering_datum\", \"begin_incident\",]]\n",
    "\n",
    "\n",
    "patient = pat[[\"PseudoID\", \"Leeftijd_startdatum_dossier\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for na values\n",
    "admission.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission['AfdelingOmschrijving'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na values in admission\n",
    "# remove incomplete admissions \n",
    "admission = admission[admission.OpnamestatusOmschrijving == \"Ontslagen\"]\n",
    "\n",
    "# change Opnamedatum Ontslagdatum to date times        \n",
    "admission[\"OpnamedatumTijd\"] = pd.to_datetime(admission[\"Opnamedatum\"] + ' ' + admission[\"Opnametijd\"])\n",
    "admission[\"OntslagdatumTijd\"] = pd.to_datetime(admission[\"Ontslagdatum\"] + ' ' + admission[\"Ontslagtijd\"])\n",
    "\n",
    "# DateTime checks for the agression and the dbc\n",
    "# these datetimes make sure it only covers the \n",
    "# If the duration of admission is less than the time check, it will take the whole admission\n",
    "admission[\"DaysF\"] = np.where(admission[\"Duur\"]>= 14, 14, admission[\"Duur\"])\n",
    "admission[\"DaysP\"] = np.where(admission[\"Duur\"]>= 3, 3, admission[\"Duur\"])\n",
    "\n",
    "# create date time checks #these should have a max value\n",
    "admission[\"DateTimeCheckF\"] = admission[\"OpnamedatumTijd\"] + pd.to_timedelta(admission[\"DaysF\"], unit='d')\n",
    "admission[\"DateTimeCheckP\"] = admission[\"OpnamedatumTijd\"] + pd.to_timedelta(admission[\"DaysP\"], unit='d')\n",
    "\n",
    "# create date time checks + 14 days\n",
    "#admission[\"DateTimeCheckFTwoWeeks\"] = admission[\"DateTimeCheckF\"] + \n",
    "\n",
    "\n",
    "\n",
    "afd = [\"Klin. Affectieve & Psychotische stoorn.\",\"Klinische Acuut & Intensieve Zorg\",\"Klin.Acuut & Intensieve. Zorg Jeugd\", \"Klin Diagn & Vroege Psychose\"]\n",
    "\n",
    "# AfdelingOmschrijving\n",
    "adm_afd = pd.get_dummies(admission[\"AfdelingOmschrijving\"])\n",
    "adm_afd_sel = pd.concat([admission, adm_afd[afd]], axis=1)\n",
    "\n",
    "del adm_afd_sel[\"AfdelingOmschrijving\"]\n",
    "\n",
    "adm_afd_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create admission1 - 8663, admission2 - 3192 and admission3 - 4685\n",
    "admission1 = adm_afd_sel.copy()\n",
    "admission2 = adm_afd_sel[admission[\"Duur\"]> 14].reset_index().copy()\n",
    "admission3 = adm_afd_sel[admission[\"Duur\"]> 3].reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check na values in dbc\n",
    "dbc.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na values in the dbc\n",
    "# select only dbc from opnamens\n",
    "# dbc = dbc[dbc.Opname == \"Ja\"]\n",
    "# PABLO: I wonder: is this necessary? If you don't do it, and you merge on PseudoID and date, what happens?\n",
    "\n",
    "# change NaN in hoofddiagnose_groep to \"Lege hoofddiagnose\" as this is already a variable in the table with the same meaning\n",
    "dbc[\"hoofddiagnose_groep\"] = dbc[\"hoofddiagnose_groep\"].replace(np.nan, \"Lege hoofddiagnose\", regex=True)\n",
    "dbc[\"hoofddiagnose_groep\"] = dbc[\"hoofddiagnose_groep\"].str.replace(\"Bijkomende codes/geen diagnose\",\"Lege hoofddiagnose\")\n",
    "\n",
    "# create a diagnose date\n",
    "# this is a limitation to be mentioned in the paper\n",
    "def get_diagnosis_date(row):\n",
    "    if type(row.DiagnoseDatum) == str:\n",
    "        return row.DiagnoseDatum\n",
    "    elif type(row.Einddatum) == str:\n",
    "        return row.Einddatum\n",
    "    else:\n",
    "        return row.Startdatum\n",
    "\n",
    "dbc[\"diagnosis_date\"] = dbc.apply(lambda row: get_diagnosis_date(row), 1)\n",
    "\n",
    "dbc.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "violent.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na values violent\n",
    "violent = violent.dropna(subset=[\"PseudoID\"])\n",
    "\n",
    "# change hantering_datum to date time with begin_incident\n",
    "violent[\"hantering_datumTijd\"] = pd.to_datetime(violent[\"hantering_datum\"] + ' ' + violent[\"begin_incident\"])\n",
    "\n",
    "violent.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN in patient\n",
    "patient.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "administering.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are only interested in administered medicine\n",
    "administering = administering[administering[\"Toegediend\"]==1]\n",
    "\n",
    "# list of agreed upon tranquilizers\n",
    "AllTranq = [\"lorazepam\", \"oxazepam\",\"diazepam\", \"zopiclone\", \"temazepam\" , \"midazolam\" , \"zolpidem\" , \"alprazolam\",\n",
    "       \"bromazepam\", \"lormetazepam\", \"clorazepate potassium\", \"nitrazepam\", \"flurazepam\", \"clobazam\", \"chlordiazepoxide\", \"brotizolam\"]\n",
    "\n",
    "administering = administering[administering[\"ATC_code_omschr\"].isin(AllTranq)]\n",
    "\n",
    "administering.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 administering does not contain a toediendatum and toedientijd (corrupted data)\n",
    "administering = administering.dropna(subset=[\"ToedienDatum\"])\n",
    "administering.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime\n",
    "administering[\"ToedienDatumTijd\"] = pd.to_datetime(administering[\"ToedienDatum\"] + ' ' + administering[\"ToedienTijd\"])\n",
    "\n",
    "administering.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge administering\n",
    "# based on the dosis\n",
    "TranqDict = {\n",
    "    \"lorazepam\": 5,\n",
    "    \"oxazepam\": 0.33,\n",
    "    \"diazepam\": 1,\n",
    "    \"zopiclone\": 1.33,\n",
    "    \"temazepam\": 1,\n",
    "    \"midazolam\": 10,\n",
    "    \"zolpidem\": 1,\n",
    "    \"alprazolam\": 10,\n",
    "    \"bromazepam\": 1,\n",
    "    \"lormetazepam\": 10,\n",
    "    \"clorazepate potassium\": 0.75,\n",
    "    \"nitrazepam\": 1,\n",
    "    \"flurazepam\": 0.33,\n",
    "    \"clobazam\": 0.5,\n",
    "    \"chlordiazepoxide\": 0.5,\n",
    "    \"brotizolam\": 40,\n",
    "}\n",
    "\n",
    "def InDiazepam(row):\n",
    "    tranquilizer_multiplier = TranqDict\n",
    "    omschr = row['ATC_code_omschr']\n",
    "    dosis = row['Dosis']\n",
    "    return tranquilizer_multiplier[omschr] * dosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "administering[\"DoseDiazepam\"] = administering.apply(InDiazepam, axis=1)\n",
    "\n",
    "administering.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set base\n",
    "# prediction over the course of all the completed cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first add the patient data to the opnamens\n",
    "def get_adm_pat(frame ):\n",
    "    adm_adm1 = frame.merge(patient, on=\"PseudoID\", how=\"left\")\n",
    "    assert np.sum(adm_adm1.isnull().sum().values) == 0\n",
    "    assert len(adm_adm1) == len(frame)\n",
    "    return adm_adm1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge patient data with admission data\n",
    "adm_pat1 = get_adm_pat(admission1)  \n",
    "\n",
    "adm_pat2 = get_adm_pat(admission2)\n",
    "\n",
    "adm_pat3 = get_adm_pat(admission3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge map data, this is where the datasets start to differ\n",
    "# give map data an unique identifier\n",
    "violent[\"IncidentID\"] = np.arange(len(violent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataset with mapdata\n",
    "map_adm1 = adm_pat1.merge(violent[[\"PseudoID\", \"IncidentID\", \"hantering_datumTijd\"]], how=\"left\", on=\"PseudoID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# whole dataset\n",
    "opname_ids, incidents_during_admission, incidents_before_admission = [], [], []\n",
    "for opname_id, grp in map_adm1.groupby(\"OpnameID\"):\n",
    "    # opname_id -> single OpnameID from adm_map table                                                                                                                                                      \n",
    "    # grp -> a dataframe containing only the rows that have OpnameID == opname_id                                                                                                                          \n",
    "    opname_ids.append(opname_id)\n",
    "    if len(grp) == 1 and len(grp[grp[\"IncidentID\"].notnull()]) == 0:\n",
    "        ''' PABLO: you can remove the condition len(grp) == 1. \n",
    "                What you need is groups for which there are no non-null incident IDs'''\n",
    "        # No incidents                                                                                                                                                                                     \n",
    "        incidents_during_admission.append(0)\n",
    "        incidents_before_admission.append(0)\n",
    "    else:\n",
    "        # At least one incident                                                                                                                                                                                                                                                                                                                                    \n",
    "        n_during = len(grp[grp.apply(lambda row: row.OntslagdatumTijd >= row.hantering_datumTijd and row.OpnamedatumTijd <= row.hantering_datumTijd, 1)])\n",
    "        n_before = len(grp[grp.apply(lambda row: row.hantering_datumTijd <= row.OpnamedatumTijd, 1)])\n",
    "        incidents_during_admission.append(n_during)\n",
    "        incidents_before_admission.append(n_before)\n",
    "\n",
    "# Create a new incidents-counts dataframe\n",
    "admission_incidents = pd.DataFrame()\n",
    "admission_incidents[\"OpnameID\"] = opname_ids\n",
    "admission_incidents[\"incidents_during_admission\"] = incidents_during_admission\n",
    "admission_incidents[\"incidents_before_admission\"] = incidents_before_admission\n",
    "assert len(admission_incidents) == len(adm_pat1)\n",
    "\n",
    "# Merge the incidents-counts dataframe onto opnames\n",
    "adm_map1 = adm_pat1.merge(admission_incidents, on=\"OpnameID\", how=\"inner\")\n",
    "assert len(adm_map1) == len(adm_pat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adm_map1\n",
    "adm_map1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_adm2 = adm_pat2.merge(violent[[\"PseudoID\", \"IncidentID\", \"hantering_datumTijd\"]], how=\"left\", on=\"PseudoID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dataset2 \n",
    "opname_ids, incidents_during_admission, incidents_before_admission = [], [], []\n",
    "for opname_id, grp in map_adm2.groupby(\"OpnameID\"):\n",
    "    # opname_id -> single OpnameID from adm_map table                                                                                                                                                      \n",
    "    # grp -> a dataframe containing only the rows that have OpnameID == opname_id                                                                                                                          \n",
    "    opname_ids.append(opname_id)\n",
    "    if len(grp) == 1 and len(grp[grp[\"IncidentID\"].notnull()]) == 0:\n",
    "        ''' PABLO: you can remove the condition len(grp) == 1. \n",
    "                What you need is groups for which there are no non-null incident IDs'''\n",
    "        # No incidents                                                                                                                                                                                     \n",
    "        incidents_during_admission.append(0)\n",
    "        incidents_before_admission.append(0)\n",
    "    else:\n",
    "        # At least one incident                                                                                                                                                                                                                                                                                                                                    \n",
    "        n_during = len(grp[grp.apply(lambda row: row.DateTimeCheckF >= row.hantering_datumTijd and row.OpnamedatumTijd <= row.hantering_datumTijd, 1)])\n",
    "        n_before = len(grp[grp.apply(lambda row: row.hantering_datumTijd <= row.OpnamedatumTijd, 1)])\n",
    "        incidents_during_admission.append(n_during)\n",
    "        incidents_before_admission.append(n_before)\n",
    "\n",
    "# Create a new incidents-counts dataframe\n",
    "admission_incidents = pd.DataFrame()\n",
    "admission_incidents[\"OpnameID\"] = opname_ids\n",
    "admission_incidents[\"incidents_during_admission\"] = incidents_during_admission\n",
    "admission_incidents[\"incidents_before_admission\"] = incidents_before_admission\n",
    "assert len(admission_incidents) == len(adm_pat2)\n",
    "\n",
    "# Merge the incidents-counts dataframe onto opnames\n",
    "adm_map2 = adm_pat2.merge(admission_incidents, on=\"OpnameID\", how=\"inner\")\n",
    "assert len(adm_map2) == len(adm_pat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adm_map2\n",
    "adm_map2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_adm3 = adm_pat3.merge(violent[[\"PseudoID\", \"IncidentID\", \"hantering_datumTijd\"]], how=\"left\", on=\"PseudoID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  dataset3 \n",
    "opname_ids, incidents_during_admission, incidents_before_admission = [], [], []\n",
    "for opname_id, grp in map_adm3.groupby(\"OpnameID\"):\n",
    "    # opname_id -> single OpnameID from adm_map table                                                                                                                                                      \n",
    "    # grp -> a dataframe containing only the rows that have OpnameID == opname_id                                                                                                                          \n",
    "    opname_ids.append(opname_id)\n",
    "    if len(grp) == 1 and len(grp[grp[\"IncidentID\"].notnull()]) == 0:\n",
    "        ''' PABLO: you can remove the condition len(grp) == 1. \n",
    "                What you need is groups for which there are no non-null incident IDs'''\n",
    "        # No incidents                                                                                                                                                                                     \n",
    "        incidents_during_admission.append(0)\n",
    "        incidents_before_admission.append(0)\n",
    "    else:\n",
    "        # At least one incident                                                                                                                                                                                                                                                                                                                                    \n",
    "        n_during = len(grp[grp.apply(lambda row: row.DateTimeCheckP >= row.hantering_datumTijd and row.OpnamedatumTijd <= row.hantering_datumTijd, 1)])\n",
    "        n_before = len(grp[grp.apply(lambda row: row.hantering_datumTijd <= row.OpnamedatumTijd, 1)])\n",
    "        incidents_during_admission.append(n_during)\n",
    "        incidents_before_admission.append(n_before)\n",
    "\n",
    "# Create a new incidents-counts dataframe\n",
    "admission_incidents = pd.DataFrame()\n",
    "admission_incidents[\"OpnameID\"] = opname_ids\n",
    "admission_incidents[\"incidents_during_admission\"] = incidents_during_admission\n",
    "admission_incidents[\"incidents_before_admission\"] = incidents_before_admission\n",
    "assert len(admission_incidents) == len(adm_pat3)\n",
    "\n",
    "# Merge the incidents-counts dataframe onto opnames\n",
    "adm_map3 = adm_pat3.merge(admission_incidents, on=\"OpnameID\", how=\"inner\")\n",
    "assert len(adm_map3) == len(adm_pat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adm_map3\n",
    "adm_map3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge dbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to merge adm_dbc with the right dbc\n",
    "\n",
    "hoofddiagnoses = [\n",
    "    \"Aandachtsstoornis\",\n",
    "    \"Andere problemen die een reden van zorg kunnen zijn\",\n",
    "    \"Angststoornissen\",\n",
    "    \"Autismespectrumstoornis\",\n",
    "    \"Bipolaire stoornissen\",\n",
    "    \"Cognitieve stoornissen\",\n",
    "    \"Depressieve stoornissen\",\n",
    "    \"Dissociatieve stoornissen\",\n",
    "    \"Gedragsstoornissen\",\n",
    "    \"Middelgerelateerde en verslavingsstoornissen\",\n",
    "    \"Obsessieve-compulsieve en verwante stoornissen\",\n",
    "    \"Overige psychische stoornissen\",\n",
    "    \"Overige stoornissen op zuigelingen of kinderleeftijd\",\n",
    "    \"Persoonlijkheidsstoornissen\",\n",
    "    \"Psychische stoornissen door een somatische aandoening\",\n",
    "    \"Schizofrenie en andere psychotische stoornissen\",\n",
    "    \"Somatisch-symptoomstoornis en verwante stoornissen\",\n",
    "    \"Trauma- en stressorgerelateerde stoornissen\",\n",
    "    \"Voedings- en eetstoornissen\",]\n",
    "\n",
    "def get_adm_dbc(dataset: int, frame ):\n",
    "    dataset_end_date_column = {1: 'OntslagdatumTijd', 2: 'DateTimeCheckF', 3: 'DateTimeCheckP'}\n",
    "    end_date = dataset_end_date_column[dataset]\n",
    "    adm_dbc = frame.merge(dbc, how='inner', on='PseudoID')\n",
    "    \n",
    "    # Opnamedatum, Ontslagdatum, Startdatum, Einddatum\n",
    "    adm_dbc[\"diagnosis_date\"]= pd.to_datetime(adm_dbc[\"diagnosis_date\"])\n",
    "    \n",
    "    adm_dbc['DbcWithin'] = adm_dbc.apply(lambda row: int(row.diagnosis_date <= row[end_date] and row.diagnosis_date >= row.OpnamedatumTijd), 1)\n",
    "    \n",
    "    # selecting only the rows that are DbcWithin\n",
    "    dbc_within = adm_dbc[adm_dbc.DbcWithin != 0]\n",
    "    \n",
    "    # create columns of interesting data, this could not be dont in one go\n",
    "    dbc_within2 = dbc_within[[\"OpnameID\", \"hoofddiagnose_groep\"]]\n",
    "    \n",
    "    # only select with a diagnoses\n",
    "    dbc_within2 = dbc_within2[dbc_within2[\"hoofddiagnose_groep\"] != \"Lege hoofddiagnose\"]\n",
    "    dbc_within2 = dbc_within2.drop_duplicates()\n",
    "    \n",
    "    # create columns of the values of hoofddiagnose_groep\n",
    "    diagnoses = pd.get_dummies(dbc_within2[\"hoofddiagnose_groep\"])\n",
    "\n",
    "    dbc_within3 = pd.concat([dbc_within2, diagnoses], axis=1)\n",
    "    \n",
    "    if \"Overige psychische stoornissen\" not in dbc_within3:\n",
    "            dbc_within3[\"Overige psychische stoornissen\"] = np.nan\n",
    "            \n",
    "    dbc_hd = pd.DataFrame(dbc_within3.groupby('OpnameID')[hoofddiagnoses].max())\n",
    "    \n",
    "    # mvpi and persoonlijkheidsstoornis\n",
    "    dbc_mp = pd.DataFrame(dbc_within.groupby('OpnameID')[['MeervoudigeProblematiekInd', \n",
    "                                                          'persoonlijkheidsstoornis']].max().reset_index())\n",
    "    \n",
    "    # zorgvraagtevraag\n",
    "    dbc_zv = pd.DataFrame(dbc_within.assign(ZorgvraagzwaarteMin = dbc_within['zorgvraagzwaarte'].abs(),\n",
    "                                            ZorgvraagzwaarteMax = dbc_within['zorgvraagzwaarte'].abs())\n",
    "                          .groupby('OpnameID')\n",
    "                          .agg({'ZorgvraagzwaarteMin':'min','ZorgvraagzwaarteMax':'max'}).reset_index())\n",
    "    assert len(dbc_mp) == dbc_within.OpnameID.nunique() and len(dbc_zv) == len(dbc_mp)\n",
    "    \n",
    "    #merge interesting columns\n",
    "    ds_dbc = dbc_mp.merge(dbc_zv, how=\"inner\", on=\"OpnameID\")\n",
    "    \n",
    "    #check that you didn't insert unnecessary rows\n",
    "    assert len(dbc_mp) == len(ds_dbc)\n",
    "    \n",
    "    #merge hoofddiagnoses\n",
    "    ds1_dbc = ds_dbc.merge(dbc_hd, how=\"inner\", on=\"OpnameID\")\n",
    "    \n",
    "    #merge interesting columns with adm_map\n",
    "    adm_dbc = frame.merge(ds1_dbc, how= \"left\", on=\"OpnameID\")\n",
    "    \n",
    "    #check that you still have the same number of admissions\n",
    "    assert len(adm_dbc) == len(frame)\n",
    "    \n",
    "    #fill na values\n",
    "    adm_dbc = adm_dbc.replace(np.nan,0)\n",
    "    \n",
    "    # test for missing values\n",
    "    assert np.sum(adm_dbc.isnull().sum().values) == 0\n",
    "    return adm_dbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dbc based on date contraints with adm_map1\n",
    "adm_dbc1 = get_adm_dbc(1,adm_map1)\n",
    "adm_dbc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dbc based on date contraints with adm_map2\n",
    "adm_dbc2 = get_adm_dbc(2,adm_map2)\n",
    "adm_dbc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dbc based on date contraints with adm_map3\n",
    "adm_dbc3 = get_adm_dbc(3,adm_map3)\n",
    "adm_dbc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge administerings dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(administering), administering.PseudoID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create past and future tranq prescriptions\n",
    "# past 3/14 days and its the same in the karin dataset\n",
    "\n",
    "var_pat = [\"PseudoID\",\n",
    "        #\"AfdelingOmschrijving\",\n",
    "        \"Spoed\",\n",
    "        \"EersteOpname\",\n",
    "        \"Geslacht\",\n",
    "        \"Leeftijd_opname\",\n",
    "        \"Duur\",\n",
    "        \"Leeftijd_startdatum_dossier\",\n",
    "        \"incidents_during_admission\",\n",
    "        \"incidents_before_admission\",\n",
    "        \"MeervoudigeProblematiekInd\",\n",
    "        \"persoonlijkheidsstoornis\",\n",
    "        \"ZorgvraagzwaarteMin\",\n",
    "        \"ZorgvraagzwaarteMax\",\n",
    "        \"DoseDiazepam\",\n",
    "        \"DoseDiazepamPre\",\n",
    "        \"DoseDiazepamPost\",   \n",
    "]\n",
    "\n",
    "UsefullVariables = var_pat + afd + hoofddiagnoses\n",
    "\n",
    "# left join on PseudoID\n",
    "# has no end date restriction for the post prescriptions\n",
    "def get_adm_dbc(dataset: int, frame ):\n",
    "    dataset_end_date_column = {1: 'OntslagdatumTijd', 2: 'DateTimeCheckF', 3: 'DateTimeCheckP'}\n",
    "    end_date = dataset_end_date_column[dataset]\n",
    "    adm_adm1 = frame.merge(administering[[\"PseudoID\", \"ToedienDatumTijd\", \"DoseDiazepam\" ]], how=\"left\", on=\"PseudoID\")  \n",
    "\n",
    "# remove rows where the ToedienDatumTijd is outside of the OpnamedatumTijd and OntslagdatumTijd\n",
    "    adm_adm1 = adm_adm1[adm_adm1[\"ToedienDatumTijd\"] >= adm_adm1[\"OpnamedatumTijd\"]]\n",
    "    adm_adm1 = adm_adm1[adm_adm1[\"ToedienDatumTijd\"] <= adm_adm1[\"OntslagdatumTijd\"]]\n",
    "\n",
    "# create past and future prescriptions\n",
    "    adm_adm1[\"Past\"] = adm_adm1.apply(lambda row: int(row.ToedienDatumTijd <= row[end_date] and row.ToedienDatumTijd >= row.OpnamedatumTijd), 1)\n",
    "\n",
    "# create two week prescriptions    \n",
    "#    adm_adm1[\"TwoWeek\"] = adm_adm1.apply(lambda row: int(row.ToedienDatumTijd <= row[end_date]+14 and row.ToedienDatumTijd >= row[end_date]), 1)\n",
    "    \n",
    "# diazepam prescribed in the past\n",
    "    adm_adm1[\"DoseDiazepamPre\"] = np.where(adm_adm1[\"Past\"]== 1, adm_adm1[\"DoseDiazepam\"], 0 )\n",
    "\n",
    "# diazepam prescribed in the twoweek\n",
    " #   adm_adm1[\"DoseDiazepamTwoWeek\"] = np.where(adm_adm1[\"TwoWeek\"]== 1, adm_adm1[\"DoseDiazepam\"], 0 )    \n",
    "    \n",
    "# diazepam prescribed in the future\n",
    "    adm_adm1[\"DoseDiazepamPost\"] = np.where(adm_adm1[\"Past\"]== 0, adm_adm1[\"DoseDiazepam\"], 0 )\n",
    "    \n",
    "# groupby\n",
    "    adm_adm2 = pd.DataFrame(adm_adm1.groupby([\"OpnameID\", \"OpnamedatumTijd\", \"OntslagdatumTijd\"])[[\"DoseDiazepam\",\"DoseDiazepamPre\",\"DoseDiazepamPost\"  ]].sum())                   \n",
    "                                      \n",
    "    Dataset = frame.merge(adm_adm2, on=[\"OpnameID\", \"OpnamedatumTijd\",\"OntslagdatumTijd\"], how=\"left\")\n",
    "\n",
    "#na values should be 0 because there are no prescriptions for tranquilizers\n",
    "    Dataset[\"DoseDiazepam\"] = Dataset[\"DoseDiazepam\"].replace(np.nan, 0, regex=True)\n",
    "    Dataset[\"DoseDiazepamPre\"] = Dataset[\"DoseDiazepamPre\"].replace(np.nan, 0, regex=True)\n",
    "    Dataset[\"DoseDiazepamPost\"] = Dataset[\"DoseDiazepamPost\"].replace(np.nan, 0, regex=True)\n",
    "\n",
    "# true dataset \n",
    "    DatasetWhole = Dataset[\n",
    "    UsefullVariables\n",
    "]\n",
    "    assert np.sum(DatasetWhole.isnull().sum().values) == 0\n",
    "    return DatasetWhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetWhole = get_adm_dbc(1 , adm_dbc1)\n",
    "\n",
    "del DatasetWhole[\"DoseDiazepamPost\"]\n",
    "del DatasetWhole[\"DoseDiazepamPre\"]\n",
    "\n",
    "DatasetWhole#.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset14Days = get_adm_dbc(2 , adm_dbc2)\n",
    "\n",
    "del Dataset14Days[\"DoseDiazepam\"]\n",
    "\n",
    "Dataset14Days#.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset3Days = get_adm_dbc(3 , adm_dbc3)\n",
    "\n",
    "del Dataset3Days[\"DoseDiazepam\"]\n",
    "\n",
    "Dataset3Days#.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously assembled 14-day dataset\n",
    "old_14_day_dataset = pd.read_csv(DATA_DIR + 'Dataset14Days.csv')\n",
    "assert len(old_14_day_dataset) == len(Dataset14Days)\n",
    "assert set([el for el in Dataset14Days.columns]) == set([el for el in old_14_day_dataset.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dict, new_dict = [{lbl: len(grp) for lbl, grp in df.groupby('PseudoID')} for df in (old_14_day_dataset, Dataset14Days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(old_dict) == len(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert old_dict.keys() == new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len([el for el in old_dict.keys() if old_dict[el] != new_dict[el]]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset14Days['PabloID'] = Dataset14Days.apply(lambda row: '_'.join([str(int(row[el])) for el in ('PseudoID', 'Duur', 'Leeftijd_opname', 'EersteOpname', 'Klin.Acuut & Intensieve. Zorg Jeugd', 'DoseDiazepamPre')]), 1)\n",
    "old_14_day_dataset['PabloID'] = old_14_day_dataset.apply(lambda row: '_'.join([str(int(row[el])) for el in ('PseudoID', 'Duur', 'Leeftijd_opname', 'EersteOpname', 'Klin.Acuut & Intensieve. Zorg Jeugd', 'DoseDiazepamPre')]), 1)\n",
    "assert len(Dataset14Days) == Dataset14Days['PabloID'].nunique()\n",
    "assert len(old_14_day_dataset) == old_14_day_dataset['PabloID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = Dataset14Days.merge(old_14_day_dataset, on='PabloID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(merged) == len(Dataset14Days)\n",
    "assert len(merged) == len(old_14_day_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_columns = ['DoseDiazepamPre', 'DoseDiazepamPost']\n",
    "for col in special_columns:\n",
    "    assert len(merged[merged.apply(lambda row: row[col + '_x'] == row[col + '_y'] or abs(row[col + '_x'] - row[col + '_y']) * 2 / (row[col + '_x'] + row[col + '_y']) < 0.005, 1)]) == len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    if col in special_columns:\n",
    "        continue\n",
    "    assert len(merged[merged.apply(lambda row: row[col + '_x'] != row[col + '_y'], 1)]) == 0, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we've gotten this far, then this notebook reproduces Jesse's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
